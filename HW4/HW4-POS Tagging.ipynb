{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Neural POS Tagger\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
    "\n",
    "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
    "- Neural POS Tagging with Viterbi / Marginal CRF\n",
    "\n",
    "Pretrained word embeddding are already given for you to use (albeit, a very bad one). Optionally, you can use your best pretrained word embeddding from previous exercise.\n",
    "\n",
    "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
    "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
    "We also create a word vector for unknown word by random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data.orchid_corpus import get_sentences\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import keras.preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
     ]
    }
   ],
   "source": [
    "unk_emb =np.random.randn(32)\n",
    "train_data = get_sentences('train')\n",
    "test_data = get_sentences('test')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open('basic_ff_embedding.pt', 'rb')\n",
    "embeddings = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "idx_to_word ={}\n",
    "label_to_idx = {}\n",
    "for sentence in train_data:\n",
    "    for word,pos in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)+1\n",
    "            idx_to_word[word_to_idx[word]] = word\n",
    "        if pos not in label_to_idx:\n",
    "            label_to_idx[pos] = len(label_to_idx)+1\n",
    "word_to_idx['UNK'] = len(word_to_idx)\n",
    "\n",
    "n_classes = len(label_to_idx.keys())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29, 327,   5, 328])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_data[100], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 362 ms, sys: 15.3 ms, total: 377 ms\n",
      "Wall time: 376 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
    "\n",
    "evaluation_report is the same as in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputToLabel(yt,seq_len):\n",
    "    out = []\n",
    "    for i in range(0,len(yt)):\n",
    "        if(i==seq_len):\n",
    "            break\n",
    "        out.append(np.argmax(yt[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "    \n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    accuracy = (all_correct / all_count) * 100\n",
    "            \n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "        \n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is this section is separated to two groups\n",
    "\n",
    "- Neural POS Tagger (4.1)\n",
    "- Neural CRF POS Tagger (4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Neural POS Tagger  (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 15,600\n",
      "Non-trainable params: 480,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx), 32, input_length=102, mask_zero=True, weights=[np.array(pre_em)], trainable=False))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 61s - loss: 2.4416 - categorical_accuracy: 0.3045    \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 61s - loss: 2.1849 - categorical_accuracy: 0.3392    \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 61s - loss: 2.1038 - categorical_accuracy: 0.3610    \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 61s - loss: 2.0235 - categorical_accuracy: 0.3910    \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 61s - loss: 1.9109 - categorical_accuracy: 0.4411    \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 61s - loss: 1.7884 - categorical_accuracy: 0.4844    \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 61s - loss: 1.6992 - categorical_accuracy: 0.5110    \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 61s - loss: 1.6319 - categorical_accuracy: 0.5318    \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 61s - loss: 1.5760 - categorical_accuracy: 0.5485    \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 62s - loss: 1.5297 - categorical_accuracy: 0.5628    \n",
      "CPU times: user 28min 20s, sys: 4min 24s, total: 32min 45s\n",
      "Wall time: 10min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7907c364e0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>93.2294</td>\n",
       "      <td>99.0231</td>\n",
       "      <td>96.039</td>\n",
       "      <td>3649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>54.6838</td>\n",
       "      <td>59.2386</td>\n",
       "      <td>56.8702</td>\n",
       "      <td>4886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>49.3192</td>\n",
       "      <td>60.0509</td>\n",
       "      <td>54.1585</td>\n",
       "      <td>10142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55.8613</td>\n",
       "      <td>87.9895</td>\n",
       "      <td>68.3375</td>\n",
       "      <td>11370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>93.0233</td>\n",
       "      <td>76.9601</td>\n",
       "      <td>84.2327</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>84.2105</td>\n",
       "      <td>3.85542</td>\n",
       "      <td>7.37327</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>79.1086</td>\n",
       "      <td>35.4557</td>\n",
       "      <td>48.9655</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>49.9587</td>\n",
       "      <td>33.6672</td>\n",
       "      <td>40.2261</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>67.1845</td>\n",
       "      <td>70.1221</td>\n",
       "      <td>68.6219</td>\n",
       "      <td>3849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>43.3333</td>\n",
       "      <td>1.15865</td>\n",
       "      <td>2.25694</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>51.1358</td>\n",
       "      <td>40.2998</td>\n",
       "      <td>45.0757</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>62.1779</td>\n",
       "      <td>64.7619</td>\n",
       "      <td>63.4436</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>71.5517</td>\n",
       "      <td>32.8713</td>\n",
       "      <td>45.0475</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>46.6667</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.865266</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>45.5336</td>\n",
       "      <td>40.8221</td>\n",
       "      <td>43.0493</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>69.2982</td>\n",
       "      <td>34.5355</td>\n",
       "      <td>46.0977</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>82.4053</td>\n",
       "      <td>65.8363</td>\n",
       "      <td>73.1949</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=57.83</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision    recall   f_score correct_count\n",
       "0                1   93.2294   99.0231    96.039          3649\n",
       "1                2   54.6838   59.2386   56.8702          4886\n",
       "2                3   49.3192   60.0509   54.1585         10142\n",
       "3                4   55.8613   87.9895   68.3375         11370\n",
       "4                5         -         0         -             0\n",
       "5                6         0         0         -             0\n",
       "6                7   93.0233   76.9601   84.2327          1600\n",
       "7                8   84.2105   3.85542   7.37327            16\n",
       "8                9         -         0         -             0\n",
       "9               10         -         0         -             0\n",
       "10              11         -         0         -             0\n",
       "11              12   79.1086   35.4557   48.9655           284\n",
       "12              13   49.9587   33.6672   40.2261          1210\n",
       "13              14   67.1845   70.1221   68.6219          3849\n",
       "14              15   43.3333   1.15865   2.25694            13\n",
       "15              16   51.1358   40.2998   45.0757           968\n",
       "16              17         0         0         -             0\n",
       "17              18   62.1779   64.7619   63.4436           748\n",
       "18              19         -         0         -             0\n",
       "19              20         -         0         -             0\n",
       "20              21   71.5517   32.8713   45.0475           498\n",
       "21              22   46.6667  0.436681  0.865266             7\n",
       "22              23   45.5336   40.8221   43.0493           576\n",
       "23              24   69.2982   34.5355   46.0977           316\n",
       "24              25         -         0         -             0\n",
       "25              26         -         0         -             0\n",
       "26              27         -         0         -             0\n",
       "27              29         -         0         -             0\n",
       "28              30         -         0         -             0\n",
       "29              31         -         0         -             0\n",
       "30              32         -         0         -             0\n",
       "31              33         -         0         -             0\n",
       "32              34   82.4053   65.8363   73.1949           370\n",
       "33              35         -         0         -             0\n",
       "34              36         -         0         -             0\n",
       "35              37         -         0         -             0\n",
       "36              38         -         0         -             0\n",
       "37              39         -         0         -             0\n",
       "38              40         -         0         -             0\n",
       "39              41         -         0         -             0\n",
       "40              42         -         0         -             0\n",
       "41              43         -         0         -             0\n",
       "42              45         -         0         -             0\n",
       "43              46         -         0         -             0\n",
       "44  accuracy=57.83                                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.2 s, sys: 7.43 s, total: 51.6 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/my_pos_no_crf.h5')\n",
    "# model.load_weights('/data/my_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Neural POS Tagger - Fix Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 1\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "(You may want to read about Keras's Masking layer)\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 ms, sys: 170 µs, total: 28.4 ms\n",
      "Wall time: 27.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write your code here\n",
    "pre_em = []\n",
    "pre_em.append(np.zeros(32))\n",
    "for i in range(1,len(idx_to_word)+1):\n",
    "    if(idx_to_word[i] in embeddings.keys()):\n",
    "        pre_em.append(embeddings[idx_to_word[i]])\n",
    "    else:\n",
    "        pre_em.append(np.zeros(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 2\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus from previous homework.\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx), 32, input_length=102, mask_zero=True, weights=[np.array(pre_em)], trainable=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 33s - loss: 1.5628 - categorical_accuracy: 0.6395    \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.9794 - categorical_accuracy: 0.7935    \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.6562 - categorical_accuracy: 0.8591    \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.4895 - categorical_accuracy: 0.8904    \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.3918 - categorical_accuracy: 0.9103    \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.3274 - categorical_accuracy: 0.9242    \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.2837 - categorical_accuracy: 0.9330    \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 34s - loss: 0.2533 - categorical_accuracy: 0.9382    \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.2313 - categorical_accuracy: 0.9420    \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 33s - loss: 0.2136 - categorical_accuracy: 0.9449    \n",
      "CPU times: user 14min 59s, sys: 2min 11s, total: 17min 10s\n",
      "Wall time: 5min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=256,epochs=10,verbose=1)\n",
    "model.save_weights('/data/my_pos_w_crf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8368</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7147</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>95.0721</td>\n",
       "      <td>93.5621</td>\n",
       "      <td>94.311</td>\n",
       "      <td>7717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.6411</td>\n",
       "      <td>97.3</td>\n",
       "      <td>93.3136</td>\n",
       "      <td>16433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.7134</td>\n",
       "      <td>99.6363</td>\n",
       "      <td>99.6748</td>\n",
       "      <td>12875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>89.5522</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>98.0603</td>\n",
       "      <td>87.1648</td>\n",
       "      <td>92.2921</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.1278</td>\n",
       "      <td>97.595</td>\n",
       "      <td>97.3608</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>33.9759</td>\n",
       "      <td>46.7662</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>75.8621</td>\n",
       "      <td>47.8261</td>\n",
       "      <td>58.6667</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>60.5787</td>\n",
       "      <td>39.9285</td>\n",
       "      <td>48.1322</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>89.8876</td>\n",
       "      <td>93.0233</td>\n",
       "      <td>91.4286</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.1776</td>\n",
       "      <td>97.3783</td>\n",
       "      <td>96.7742</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>87.8676</td>\n",
       "      <td>86.4496</td>\n",
       "      <td>87.1529</td>\n",
       "      <td>3107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.5606</td>\n",
       "      <td>93.1135</td>\n",
       "      <td>93.8315</td>\n",
       "      <td>5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>80.9899</td>\n",
       "      <td>64.1711</td>\n",
       "      <td>71.6062</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.1255</td>\n",
       "      <td>89.592</td>\n",
       "      <td>88.3415</td>\n",
       "      <td>2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>86.3014</td>\n",
       "      <td>91.6364</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.3662</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>98.2847</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.9104</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>96.4706</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.9619</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>92.4142</td>\n",
       "      <td>92.4752</td>\n",
       "      <td>92.4447</td>\n",
       "      <td>1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>81.8302</td>\n",
       "      <td>77.5421</td>\n",
       "      <td>79.6284</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>87.6375</td>\n",
       "      <td>95.9603</td>\n",
       "      <td>91.6103</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>80.7018</td>\n",
       "      <td>85.4645</td>\n",
       "      <td>83.0149</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>91.958</td>\n",
       "      <td>63.6804</td>\n",
       "      <td>75.2504</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>87.6543</td>\n",
       "      <td>80.6818</td>\n",
       "      <td>84.0237</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>82.4427</td>\n",
       "      <td>87.0968</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>95.0311</td>\n",
       "      <td>96.8354</td>\n",
       "      <td>95.9248</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>75</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>72.7273</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>53.9773</td>\n",
       "      <td>92.233</td>\n",
       "      <td>68.1004</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>33.7079</td>\n",
       "      <td>49.7925</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>88.1956</td>\n",
       "      <td>93.0605</td>\n",
       "      <td>90.5628</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.991</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.8357</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>100</td>\n",
       "      <td>2.5641</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>78.2609</td>\n",
       "      <td>77.1429</td>\n",
       "      <td>77.6978</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>99.2857</td>\n",
       "      <td>99.6416</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>17.6471</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.73</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8368  99.5929  99.7147          3670\n",
       "1                2   95.0721  93.5621   94.311          7717\n",
       "2                3   89.6411     97.3  93.3136         16433\n",
       "3                4   99.7134  99.6363  99.6748         12875\n",
       "4                5   95.2381  89.5522  92.3077            60\n",
       "5                6   98.0603  87.1648  92.2921           455\n",
       "6                7   97.1278   97.595  97.3608          2029\n",
       "7                8        75  33.9759  46.7662           141\n",
       "8                9   75.8621  47.8261  58.6667           176\n",
       "9               10   60.5787  39.9285  48.1322           335\n",
       "10              11   89.8876  93.0233  91.4286            80\n",
       "11              12   96.1776  97.3783  96.7742           780\n",
       "12              13   87.8676  86.4496  87.1529          3107\n",
       "13              14   94.5606  93.1135  93.8315          5111\n",
       "14              15   80.9899  64.1711  71.6062           720\n",
       "15              16   87.1255   89.592  88.3415          2152\n",
       "16              17   97.6744  86.3014  91.6364           504\n",
       "17              18   97.3662  99.2208  98.2847          1146\n",
       "18              19   97.9104  95.0725  96.4706           328\n",
       "19              20   98.9619  96.9492  97.9452           286\n",
       "20              21   92.4142  92.4752  92.4447          1401\n",
       "21              22   81.8302  77.5421  79.6284          1243\n",
       "22              23   87.6375  95.9603  91.6103          1354\n",
       "23              24   80.7018  85.4645  83.0149           782\n",
       "24              25    91.958  63.6804  75.2504           263\n",
       "25              26   87.6543  80.6818  84.0237           142\n",
       "26              27   92.3077  82.4427  87.0968           108\n",
       "27              29   95.0311  96.8354  95.9248           306\n",
       "28              30        75  70.5882  72.7273            72\n",
       "29              31   53.9773   92.233  68.1004            95\n",
       "30              32   95.2381  33.7079  49.7925            60\n",
       "31              33         -        0        -             0\n",
       "32              34   88.1956  93.0605  90.5628           523\n",
       "33              35         -        0        -             0\n",
       "34              36         -        0        -             0\n",
       "35              37    90.991  99.0196  94.8357           101\n",
       "36              38       100   2.5641        5             1\n",
       "37              39   78.2609  77.1429  77.6978           108\n",
       "38              40       100  99.2857  99.6416           278\n",
       "39              41         -        0        -             0\n",
       "40              42       100  17.6471       30             3\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.73                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.6 s, sys: 6.85 s, total: 51.4 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.load_weights('/data/my_pos_w_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 3\n",
    "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform best or worst, why?)\n",
    "\n",
    "(If you use your own weight please state so in the answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>\n",
    "<pre style=\"background-color: lightgreen\">\n",
    "Since pre-trained weight has already been set up to better initial condition from pre-trained words, it performed better in accuracy with 92.73 to 57.83 accuracy. Also the non-pretrained has set `Trainable` to `False`, so the weight is fixed at random vaule. Eventhough the overall network perform better as more epoch, the model cannot be changed. Thus lead to the lower accuracy rate.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 CRF Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
    "\n",
    "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 4\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Do not forget to save this model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 102, 48)           4752      \n",
      "=================================================================\n",
      "Total params: 500,960\n",
      "Trainable params: 500,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation = 'relu')))\n",
    "crf = CRF(n_classes,\n",
    "          learn_mode='join',\n",
    "          test_mode='viterbi',\n",
    "          sparse_target=False,\n",
    "          use_boundary=True,\n",
    "          use_bias=True,\n",
    "          activation='linear',\n",
    "          kernel_initializer='glorot_uniform',\n",
    "          chain_initializer='orthogonal',\n",
    "          bias_initializer='zeros',\n",
    "          boundary_initializer='zeros',\n",
    "          kernel_regularizer=None,\n",
    "          chain_regularizer=None,\n",
    "          boundary_regularizer=None,\n",
    "          bias_regularizer=None,\n",
    "          kernel_constraint=None,\n",
    "          chain_constraint=None,\n",
    "          boundary_constraint=None,\n",
    "          bias_constraint=None,\n",
    "          input_dim=None,\n",
    "          unroll=False)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.0015)\n",
    "model.compile(optimizer=adam,loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/5\n",
      "14800/14800 [==============================] - 71s - loss: 46.9092 - acc: 0.4141 - val_loss: 44.2042 - val_acc: 0.7787\n",
      "Epoch 2/5\n",
      "14800/14800 [==============================] - 71s - loss: 45.1785 - acc: 0.8762 - val_loss: 43.6239 - val_acc: 0.8971\n",
      "Epoch 3/5\n",
      "14800/14800 [==============================] - 71s - loss: 44.9055 - acc: 0.9289 - val_loss: 43.5473 - val_acc: 0.9093\n",
      "Epoch 4/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.8405 - acc: 0.9409 - val_loss: 43.5197 - val_acc: 0.9157\n",
      "Epoch 5/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.8113 - acc: 0.9469 - val_loss: 43.5070 - val_acc: 0.9172\n",
      "CPU times: user 15min 57s, sys: 2min 17s, total: 18min 15s\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=5,verbose=1,shuffle=True,validation_split=0.2,\n",
    "         callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>91.9514</td>\n",
       "      <td>94.6048</td>\n",
       "      <td>93.2592</td>\n",
       "      <td>7803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.1332</td>\n",
       "      <td>94.5704</td>\n",
       "      <td>92.82</td>\n",
       "      <td>15972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9922</td>\n",
       "      <td>99.5976</td>\n",
       "      <td>99.7945</td>\n",
       "      <td>12870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>86.8421</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>98.6957</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.4644</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.105</td>\n",
       "      <td>97.114</td>\n",
       "      <td>97.607</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>71.6049</td>\n",
       "      <td>27.9518</td>\n",
       "      <td>40.208</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>73.8351</td>\n",
       "      <td>55.9783</td>\n",
       "      <td>63.6785</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>61.1111</td>\n",
       "      <td>40.6436</td>\n",
       "      <td>48.8189</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>92.0455</td>\n",
       "      <td>94.186</td>\n",
       "      <td>93.1034</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>93.3254</td>\n",
       "      <td>97.7528</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>88.1933</td>\n",
       "      <td>85.8375</td>\n",
       "      <td>86.9994</td>\n",
       "      <td>3085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.2868</td>\n",
       "      <td>94.407</td>\n",
       "      <td>94.3468</td>\n",
       "      <td>5182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>64.7569</td>\n",
       "      <td>66.4884</td>\n",
       "      <td>65.6113</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>86.3019</td>\n",
       "      <td>89.9667</td>\n",
       "      <td>88.0962</td>\n",
       "      <td>2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.8641</td>\n",
       "      <td>86.3014</td>\n",
       "      <td>91.7197</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.7089</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>97.9487</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.9104</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>96.4706</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.9619</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>95.2876</td>\n",
       "      <td>90.7591</td>\n",
       "      <td>92.9682</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>79.8949</td>\n",
       "      <td>75.8578</td>\n",
       "      <td>77.824</td>\n",
       "      <td>1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.8089</td>\n",
       "      <td>96.1729</td>\n",
       "      <td>92.3443</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>79.6488</td>\n",
       "      <td>84.2623</td>\n",
       "      <td>81.8906</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>86.0058</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>78.0423</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>88.8268</td>\n",
       "      <td>90.3409</td>\n",
       "      <td>89.5775</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>88.1356</td>\n",
       "      <td>79.3893</td>\n",
       "      <td>83.5341</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.7492</td>\n",
       "      <td>97.1519</td>\n",
       "      <td>94.8995</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>78.125</td>\n",
       "      <td>73.5294</td>\n",
       "      <td>75.7576</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>56.6879</td>\n",
       "      <td>86.4078</td>\n",
       "      <td>68.4615</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>95.5224</td>\n",
       "      <td>35.9551</td>\n",
       "      <td>52.2449</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>81.1321</td>\n",
       "      <td>91.8149</td>\n",
       "      <td>86.1436</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>11.1111</td>\n",
       "      <td>16.6667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>91.8182</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>95.283</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>58.6207</td>\n",
       "      <td>43.5897</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>73.8372</td>\n",
       "      <td>90.7143</td>\n",
       "      <td>81.4103</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>99.6429</td>\n",
       "      <td>99.8211</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "      <td>26.087</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>17.6471</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.35</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   91.9514  94.6048  93.2592          7803\n",
       "2                3   91.1332  94.5704    92.82         15972\n",
       "3                4   99.9922  99.5976  99.7945         12870\n",
       "4                5   86.8421  98.5075  92.3077            66\n",
       "5                6   98.6957  86.9732  92.4644           454\n",
       "6                7    98.105   97.114   97.607          2019\n",
       "7                8   71.6049  27.9518   40.208           116\n",
       "8                9   73.8351  55.9783  63.6785           206\n",
       "9               10   61.1111  40.6436  48.8189           341\n",
       "10              11   92.0455   94.186  93.1034            81\n",
       "11              12   93.3254  97.7528  95.4878           783\n",
       "12              13   88.1933  85.8375  86.9994          3085\n",
       "13              14   94.2868   94.407  94.3468          5182\n",
       "14              15   64.7569  66.4884  65.6113           746\n",
       "15              16   86.3019  89.9667  88.0962          2161\n",
       "16              17   97.8641  86.3014  91.7197           504\n",
       "17              18   96.7089  99.2208  97.9487          1146\n",
       "18              19   97.9104  95.0725  96.4706           328\n",
       "19              20   98.9619  96.9492  97.9452           286\n",
       "20              21   95.2876  90.7591  92.9682          1375\n",
       "21              22   79.8949  75.8578   77.824          1216\n",
       "22              23   88.8089  96.1729  92.3443          1357\n",
       "23              24   79.6488  84.2623  81.8906           771\n",
       "24              25   86.0058  71.4286  78.0423           295\n",
       "25              26   88.8268  90.3409  89.5775           159\n",
       "26              27   88.1356  79.3893  83.5341           104\n",
       "27              29   92.7492  97.1519  94.8995           307\n",
       "28              30    78.125  73.5294  75.7576            75\n",
       "29              31   56.6879  86.4078  68.4615            89\n",
       "30              32   95.5224  35.9551  52.2449            64\n",
       "31              33         -        0        -             0\n",
       "32              34   81.1321  91.8149  86.1436           516\n",
       "33              35   33.3333  11.1111  16.6667             1\n",
       "34              36         -        0        -             0\n",
       "35              37   91.8182  99.0196   95.283           101\n",
       "36              38   58.6207  43.5897       50            17\n",
       "37              39   73.8372  90.7143  81.4103           127\n",
       "38              40       100  99.6429  99.8211           279\n",
       "39              41       100       15   26.087             3\n",
       "40              42       100  17.6471       30             3\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.35                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7940 - acc: 0.9499 - val_loss: 43.4974 - val_acc: 0.9192\n",
      "Epoch 2/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7811 - acc: 0.9534 - val_loss: 43.4961 - val_acc: 0.9195\n",
      "Epoch 3/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7715 - acc: 0.9553 - val_loss: 43.4959 - val_acc: 0.9212\n",
      "Epoch 4/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7635 - acc: 0.9572 - val_loss: 43.4973 - val_acc: 0.9223\n",
      "Epoch 5/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7558 - acc: 0.9592 - val_loss: 43.4970 - val_acc: 0.9213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7907c36a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=128,epochs=5,verbose=1,shuffle=True,validation_split=0.2, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8639</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7283</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.6768</td>\n",
       "      <td>93.5863</td>\n",
       "      <td>94.1284</td>\n",
       "      <td>7719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.1388</td>\n",
       "      <td>95.6717</td>\n",
       "      <td>93.3503</td>\n",
       "      <td>16158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9534</td>\n",
       "      <td>99.6518</td>\n",
       "      <td>99.8024</td>\n",
       "      <td>12877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>91.6667</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>94.964</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.1266</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.6531</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.2885</td>\n",
       "      <td>96.6811</td>\n",
       "      <td>97.4782</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>72.1992</td>\n",
       "      <td>41.9277</td>\n",
       "      <td>53.0488</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>70.8738</td>\n",
       "      <td>59.5109</td>\n",
       "      <td>64.6972</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.1572</td>\n",
       "      <td>40.5244</td>\n",
       "      <td>49.062</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>91.3043</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>94.382</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>93.2384</td>\n",
       "      <td>98.1273</td>\n",
       "      <td>95.6204</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>85.8514</td>\n",
       "      <td>87.1174</td>\n",
       "      <td>86.4798</td>\n",
       "      <td>3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.8402</td>\n",
       "      <td>94.0973</td>\n",
       "      <td>94.4673</td>\n",
       "      <td>5165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>70.9282</td>\n",
       "      <td>72.1925</td>\n",
       "      <td>71.5548</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.0865</td>\n",
       "      <td>90.9659</td>\n",
       "      <td>88.9839</td>\n",
       "      <td>2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.6605</td>\n",
       "      <td>89.2123</td>\n",
       "      <td>92.7872</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.3087</td>\n",
       "      <td>99.3939</td>\n",
       "      <td>97.827</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.3607</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.793</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.1864</td>\n",
       "      <td>92.0792</td>\n",
       "      <td>92.6295</td>\n",
       "      <td>1395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.6154</td>\n",
       "      <td>74.111</td>\n",
       "      <td>79.0156</td>\n",
       "      <td>1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>90.2276</td>\n",
       "      <td>95.5351</td>\n",
       "      <td>92.8055</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>85.0273</td>\n",
       "      <td>85.0273</td>\n",
       "      <td>85.0273</td>\n",
       "      <td>778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>89.2537</td>\n",
       "      <td>72.3971</td>\n",
       "      <td>79.9465</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>93.6047</td>\n",
       "      <td>91.4773</td>\n",
       "      <td>92.5287</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>95.9184</td>\n",
       "      <td>71.7557</td>\n",
       "      <td>82.0961</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.4169</td>\n",
       "      <td>94.3038</td>\n",
       "      <td>93.8583</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>77.551</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>58.3333</td>\n",
       "      <td>81.5534</td>\n",
       "      <td>68.0162</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>65.0602</td>\n",
       "      <td>60.6742</td>\n",
       "      <td>62.7907</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>87.3264</td>\n",
       "      <td>89.5018</td>\n",
       "      <td>88.4007</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.991</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.8357</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>56.25</td>\n",
       "      <td>46.1538</td>\n",
       "      <td>50.7042</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>69.9482</td>\n",
       "      <td>96.4286</td>\n",
       "      <td>81.0811</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.87</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8639  99.5929  99.7283          3670\n",
       "1                2   94.6768  93.5863  94.1284          7719\n",
       "2                3   91.1388  95.6717  93.3503         16158\n",
       "3                4   99.9534  99.6518  99.8024         12877\n",
       "4                5   91.6667  98.5075   94.964            66\n",
       "5                6   99.1266  86.9732  92.6531           454\n",
       "6                7   98.2885  96.6811  97.4782          2010\n",
       "7                8   72.1992  41.9277  53.0488           174\n",
       "8                9   70.8738  59.5109  64.6972           219\n",
       "9               10   62.1572  40.5244   49.062           340\n",
       "10              11   91.3043  97.6744   94.382            84\n",
       "11              12   93.2384  98.1273  95.6204           786\n",
       "12              13   85.8514  87.1174  86.4798          3131\n",
       "13              14   94.8402  94.0973  94.4673          5165\n",
       "14              15   70.9282  72.1925  71.5548           810\n",
       "15              16   87.0865  90.9659  88.9839          2185\n",
       "16              17   96.6605  89.2123  92.7872           521\n",
       "17              18   96.3087  99.3939   97.827          1148\n",
       "18              19   97.3607  96.2319   96.793           332\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21   93.1864  92.0792  92.6295          1395\n",
       "21              22   84.6154   74.111  79.0156          1188\n",
       "22              23   90.2276  95.5351  92.8055          1348\n",
       "23              24   85.0273  85.0273  85.0273           778\n",
       "24              25   89.2537  72.3971  79.9465           299\n",
       "25              26   93.6047  91.4773  92.5287           161\n",
       "26              27   95.9184  71.7557  82.0961            94\n",
       "27              29   93.4169  94.3038  93.8583           298\n",
       "28              30    77.551  74.5098       76            76\n",
       "29              31   58.3333  81.5534  68.0162            84\n",
       "30              32   65.0602  60.6742  62.7907           108\n",
       "31              33         -        0        -             0\n",
       "32              34   87.3264  89.5018  88.4007           503\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100     87.5  93.3333            14\n",
       "35              37    90.991  99.0196  94.8357           101\n",
       "36              38     56.25  46.1538  50.7042            18\n",
       "37              39   69.9482  96.4286  81.0811           135\n",
       "38              40       100      100      100           280\n",
       "39              41        75       75       75            15\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.87                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/data/viterbi_crf_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('/data/viterbi_crf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/10\n",
      "14800/14800 [==============================] - 39s - loss: 47.6976 - acc: 0.2141 - val_loss: 45.3184 - val_acc: 0.4184\n",
      "Epoch 2/10\n",
      "14800/14800 [==============================] - 38s - loss: 46.0685 - acc: 0.6046 - val_loss: 44.0675 - val_acc: 0.8035\n",
      "Epoch 3/10\n",
      "14800/14800 [==============================] - 37s - loss: 45.1702 - acc: 0.8771 - val_loss: 43.6798 - val_acc: 0.8888\n",
      "Epoch 4/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.9468 - acc: 0.9220 - val_loss: 43.5998 - val_acc: 0.9045\n",
      "Epoch 5/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.8758 - acc: 0.9358 - val_loss: 43.5673 - val_acc: 0.9097\n",
      "Epoch 6/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.8432 - acc: 0.9415 - val_loss: 43.5495 - val_acc: 0.9120\n",
      "Epoch 7/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.8228 - acc: 0.9446 - val_loss: 43.5406 - val_acc: 0.9134\n",
      "Epoch 8/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.8089 - acc: 0.9471 - val_loss: 43.5314 - val_acc: 0.9157\n",
      "Epoch 9/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.7980 - acc: 0.9495 - val_loss: 43.5245 - val_acc: 0.9167\n",
      "Epoch 10/10\n",
      "14800/14800 [==============================] - 38s - loss: 44.7888 - acc: 0.9512 - val_loss: 43.5223 - val_acc: 0.9177\n",
      "CPU times: user 17min 15s, sys: 2min 23s, total: 19min 39s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=256,epochs=10,verbose=1,shuffle=True,validation_split=0.2, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.1198</td>\n",
       "      <td>92.9559</td>\n",
       "      <td>93.5342</td>\n",
       "      <td>7667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.9381</td>\n",
       "      <td>96.3763</td>\n",
       "      <td>93.046</td>\n",
       "      <td>16277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.5898</td>\n",
       "      <td>99.779</td>\n",
       "      <td>12869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>79.4521</td>\n",
       "      <td>86.5672</td>\n",
       "      <td>82.8571</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>93.0328</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.0948</td>\n",
       "      <td>96.5849</td>\n",
       "      <td>97.334</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>69.1943</td>\n",
       "      <td>35.1807</td>\n",
       "      <td>46.6454</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>73.9777</td>\n",
       "      <td>54.0761</td>\n",
       "      <td>62.4804</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>59.6078</td>\n",
       "      <td>36.2336</td>\n",
       "      <td>45.0704</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>94.9153</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>94.9214</td>\n",
       "      <td>98.0025</td>\n",
       "      <td>96.4373</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>85.746</td>\n",
       "      <td>86.0323</td>\n",
       "      <td>85.8889</td>\n",
       "      <td>3092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>95.3909</td>\n",
       "      <td>93.1317</td>\n",
       "      <td>94.2478</td>\n",
       "      <td>5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>77.2994</td>\n",
       "      <td>70.41</td>\n",
       "      <td>73.694</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>86.2823</td>\n",
       "      <td>90.3414</td>\n",
       "      <td>88.2652</td>\n",
       "      <td>2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>95.8188</td>\n",
       "      <td>94.1781</td>\n",
       "      <td>94.9914</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.2743</td>\n",
       "      <td>98.8745</td>\n",
       "      <td>98.0678</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>95.3488</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>95.2104</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.2818</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.6109</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.2796</td>\n",
       "      <td>91.6172</td>\n",
       "      <td>92.4409</td>\n",
       "      <td>1388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.136</td>\n",
       "      <td>74.111</td>\n",
       "      <td>78.806</td>\n",
       "      <td>1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.4667</td>\n",
       "      <td>95.1099</td>\n",
       "      <td>92.202</td>\n",
       "      <td>1342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>82.1235</td>\n",
       "      <td>82.8415</td>\n",
       "      <td>82.481</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>85.1003</td>\n",
       "      <td>71.9128</td>\n",
       "      <td>77.9528</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>90.2299</td>\n",
       "      <td>89.2045</td>\n",
       "      <td>89.7143</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>89.2857</td>\n",
       "      <td>76.3359</td>\n",
       "      <td>82.3045</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.2388</td>\n",
       "      <td>97.7848</td>\n",
       "      <td>94.9309</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>63.7168</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>66.9767</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>74.2574</td>\n",
       "      <td>72.8155</td>\n",
       "      <td>73.5294</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>76</td>\n",
       "      <td>53.3708</td>\n",
       "      <td>62.7063</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>86.0465</td>\n",
       "      <td>92.1708</td>\n",
       "      <td>89.0034</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>37.5</td>\n",
       "      <td>54.5455</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>93.4579</td>\n",
       "      <td>98.0392</td>\n",
       "      <td>95.6938</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>60</td>\n",
       "      <td>30.7692</td>\n",
       "      <td>40.678</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>67.0157</td>\n",
       "      <td>91.4286</td>\n",
       "      <td>77.3414</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>99.6441</td>\n",
       "      <td>100</td>\n",
       "      <td>99.8217</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>70</td>\n",
       "      <td>68.2927</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>92.8571</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>83.871</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.59</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   94.1198  92.9559  93.5342          7667\n",
       "2                3   89.9381  96.3763   93.046         16277\n",
       "3                4   99.9689  99.5898   99.779         12869\n",
       "4                5   79.4521  86.5672  82.8571            58\n",
       "5                6       100  86.9732  93.0328           454\n",
       "6                7   98.0948  96.5849   97.334          2008\n",
       "7                8   69.1943  35.1807  46.6454           146\n",
       "8                9   73.9777  54.0761  62.4804           199\n",
       "9               10   59.6078  36.2336  45.0704           304\n",
       "10              11   92.3077  97.6744  94.9153            84\n",
       "11              12   94.9214  98.0025  96.4373           785\n",
       "12              13    85.746  86.0323  85.8889          3092\n",
       "13              14   95.3909  93.1317  94.2478          5112\n",
       "14              15   77.2994    70.41   73.694           790\n",
       "15              16   86.2823  90.3414  88.2652          2170\n",
       "16              17   95.8188  94.1781  94.9914           550\n",
       "17              18   97.2743  98.8745  98.0678          1142\n",
       "18              19   95.3488  95.0725  95.2104           328\n",
       "19              20   98.2818  96.9492  97.6109           286\n",
       "20              21   93.2796  91.6172  92.4409          1388\n",
       "21              22    84.136   74.111   78.806          1188\n",
       "22              23   89.4667  95.1099   92.202          1342\n",
       "23              24   82.1235  82.8415   82.481           758\n",
       "24              25   85.1003  71.9128  77.9528           297\n",
       "25              26   90.2299  89.2045  89.7143           157\n",
       "26              27   89.2857  76.3359  82.3045           100\n",
       "27              29   92.2388  97.7848  94.9309           309\n",
       "28              30   63.7168  70.5882  66.9767            72\n",
       "29              31   74.2574  72.8155  73.5294            75\n",
       "30              32        76  53.3708  62.7063            95\n",
       "31              33         -        0        -             0\n",
       "32              34   86.0465  92.1708  89.0034           518\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100     37.5  54.5455             6\n",
       "35              37   93.4579  98.0392  95.6938           100\n",
       "36              38        60  30.7692   40.678            12\n",
       "37              39   67.0157  91.4286  77.3414           128\n",
       "38              40   99.6441      100  99.8217           280\n",
       "39              41   66.6667       70  68.2927            14\n",
       "40              42   92.8571  76.4706   83.871            13\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.59                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/5\n",
      "14800/14800 [==============================] - 39s - loss: 44.7803 - acc: 0.9535 - val_loss: 43.5166 - val_acc: 0.9192\n",
      "Epoch 2/5\n",
      "14800/14800 [==============================] - 39s - loss: 44.7750 - acc: 0.9542 - val_loss: 43.5185 - val_acc: 0.9203\n",
      "Epoch 3/5\n",
      "14800/14800 [==============================] - 39s - loss: 44.7688 - acc: 0.9561 - val_loss: 43.5191 - val_acc: 0.9204\n",
      "Epoch 4/5\n",
      "14800/14800 [==============================] - 39s - loss: 44.7628 - acc: 0.9572 - val_loss: 43.5222 - val_acc: 0.9203\n",
      "Epoch 5/5\n",
      "14800/14800 [==============================] - 39s - loss: 44.7581 - acc: 0.9586 - val_loss: 43.5196 - val_acc: 0.9204\n",
      "CPU times: user 8min 55s, sys: 1min 13s, total: 10min 9s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=256,epochs=5,verbose=1,shuffle=True,validation_split=0.2, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.3702</td>\n",
       "      <td>92.4709</td>\n",
       "      <td>93.4109</td>\n",
       "      <td>7627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90.2278</td>\n",
       "      <td>96.1632</td>\n",
       "      <td>93.101</td>\n",
       "      <td>16241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.4583</td>\n",
       "      <td>99.7129</td>\n",
       "      <td>12852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>86.9565</td>\n",
       "      <td>89.5522</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7802</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.9376</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.3831</td>\n",
       "      <td>96.5849</td>\n",
       "      <td>97.4757</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>64.5038</td>\n",
       "      <td>40.7229</td>\n",
       "      <td>49.9261</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>73.8019</td>\n",
       "      <td>62.7717</td>\n",
       "      <td>67.8414</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>58.498</td>\n",
       "      <td>35.2801</td>\n",
       "      <td>44.0149</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>89.3617</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.1395</td>\n",
       "      <td>96.3795</td>\n",
       "      <td>96.2594</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>85.3047</td>\n",
       "      <td>86.0879</td>\n",
       "      <td>85.6945</td>\n",
       "      <td>3094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.9807</td>\n",
       "      <td>94.1155</td>\n",
       "      <td>94.5461</td>\n",
       "      <td>5166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>79.7392</td>\n",
       "      <td>70.8556</td>\n",
       "      <td>75.0354</td>\n",
       "      <td>795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.1046</td>\n",
       "      <td>89.4255</td>\n",
       "      <td>88.2498</td>\n",
       "      <td>2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.3532</td>\n",
       "      <td>85.9589</td>\n",
       "      <td>90.8597</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.7824</td>\n",
       "      <td>98.961</td>\n",
       "      <td>97.8596</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.0674</td>\n",
       "      <td>95.942</td>\n",
       "      <td>96.5015</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.042</td>\n",
       "      <td>91.6832</td>\n",
       "      <td>92.8476</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>83.2224</td>\n",
       "      <td>77.9788</td>\n",
       "      <td>80.5153</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>87.5809</td>\n",
       "      <td>95.9603</td>\n",
       "      <td>91.5793</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>85.6322</td>\n",
       "      <td>81.4208</td>\n",
       "      <td>83.4734</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>88.1844</td>\n",
       "      <td>74.092</td>\n",
       "      <td>80.5263</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>91.9075</td>\n",
       "      <td>90.3409</td>\n",
       "      <td>91.1175</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>90.5172</td>\n",
       "      <td>80.1527</td>\n",
       "      <td>85.0202</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.4471</td>\n",
       "      <td>96.8354</td>\n",
       "      <td>94.5904</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>71.8447</td>\n",
       "      <td>72.549</td>\n",
       "      <td>72.1951</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>49.0446</td>\n",
       "      <td>74.7573</td>\n",
       "      <td>59.2308</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>72.9927</td>\n",
       "      <td>56.1798</td>\n",
       "      <td>63.4921</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>85.173</td>\n",
       "      <td>91.9929</td>\n",
       "      <td>88.4517</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>92.5926</td>\n",
       "      <td>98.0392</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>65.5172</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>55.8824</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>69.3548</td>\n",
       "      <td>92.1429</td>\n",
       "      <td>79.1411</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>99.6441</td>\n",
       "      <td>100</td>\n",
       "      <td>99.8217</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>88.2353</td>\n",
       "      <td>75</td>\n",
       "      <td>81.0811</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>86.6667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.64</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   94.3702  92.4709  93.4109          7627\n",
       "2                3   90.2278  96.1632   93.101         16241\n",
       "3                4   99.9689  99.4583  99.7129         12852\n",
       "4                5   86.9565  89.5522  88.2353            60\n",
       "5                6   99.7802  86.9732  92.9376           454\n",
       "6                7   98.3831  96.5849  97.4757          2008\n",
       "7                8   64.5038  40.7229  49.9261           169\n",
       "8                9   73.8019  62.7717  67.8414           231\n",
       "9               10    58.498  35.2801  44.0149           296\n",
       "10              11   89.3617  97.6744  93.3333            84\n",
       "11              12   96.1395  96.3795  96.2594           772\n",
       "12              13   85.3047  86.0879  85.6945          3094\n",
       "13              14   94.9807  94.1155  94.5461          5166\n",
       "14              15   79.7392  70.8556  75.0354           795\n",
       "15              16   87.1046  89.4255  88.2498          2148\n",
       "16              17   96.3532  85.9589  90.8597           502\n",
       "17              18   96.7824   98.961  97.8596          1143\n",
       "18              19   97.0674   95.942  96.5015           331\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21    94.042  91.6832  92.8476          1389\n",
       "21              22   83.2224  77.9788  80.5153          1250\n",
       "22              23   87.5809  95.9603  91.5793          1354\n",
       "23              24   85.6322  81.4208  83.4734           745\n",
       "24              25   88.1844   74.092  80.5263           306\n",
       "25              26   91.9075  90.3409  91.1175           159\n",
       "26              27   90.5172  80.1527  85.0202           105\n",
       "27              29   92.4471  96.8354  94.5904           306\n",
       "28              30   71.8447   72.549  72.1951            74\n",
       "29              31   49.0446  74.7573  59.2308            77\n",
       "30              32   72.9927  56.1798  63.4921           100\n",
       "31              33         -        0        -             0\n",
       "32              34    85.173  91.9929  88.4517           517\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100     87.5  93.3333            14\n",
       "35              37   92.5926  98.0392  95.2381           100\n",
       "36              38   65.5172  48.7179  55.8824            19\n",
       "37              39   69.3548  92.1429  79.1411           129\n",
       "38              40   99.6441      100  99.8217           280\n",
       "39              41   88.2353       75  81.0811            15\n",
       "40              42       100  76.4706  86.6667            13\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.64                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/data/viterbi_crf_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/5\n",
      "14800/14800 [==============================] - 69s - loss: 44.7495 - acc: 0.9609 - val_loss: 43.5045 - val_acc: 0.9210\n",
      "Epoch 2/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7435 - acc: 0.9621 - val_loss: 43.5086 - val_acc: 0.9219\n",
      "Epoch 3/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7376 - acc: 0.9634 - val_loss: 43.5114 - val_acc: 0.9199\n",
      "Epoch 4/5\n",
      "14800/14800 [==============================] - 70s - loss: 44.7331 - acc: 0.9651 - val_loss: 43.5122 - val_acc: 0.9202\n",
      "Epoch 5/5\n",
      "14800/14800 [==============================] - 71s - loss: 44.7269 - acc: 0.9666 - val_loss: 43.5234 - val_acc: 0.9200\n",
      "CPU times: user 15min 46s, sys: 2min 20s, total: 18min 7s\n",
      "Wall time: 5min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=5,verbose=1,shuffle=True,validation_split=0.2, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.891</td>\n",
       "      <td>99.5115</td>\n",
       "      <td>99.7009</td>\n",
       "      <td>3667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>93.8</td>\n",
       "      <td>93.9137</td>\n",
       "      <td>93.8568</td>\n",
       "      <td>7746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90.8744</td>\n",
       "      <td>94.3395</td>\n",
       "      <td>92.5745</td>\n",
       "      <td>15933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.3809</td>\n",
       "      <td>99.674</td>\n",
       "      <td>12842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>83.5443</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>90.411</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>96.8553</td>\n",
       "      <td>88.5057</td>\n",
       "      <td>92.4925</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>98.0507</td>\n",
       "      <td>96.7773</td>\n",
       "      <td>97.4098</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>63.1737</td>\n",
       "      <td>50.8434</td>\n",
       "      <td>56.3418</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>63.5838</td>\n",
       "      <td>59.7826</td>\n",
       "      <td>61.6246</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>59.2453</td>\n",
       "      <td>37.4255</td>\n",
       "      <td>45.8729</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>89.3617</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>92.2353</td>\n",
       "      <td>97.8777</td>\n",
       "      <td>94.9727</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>87.5036</td>\n",
       "      <td>85.1419</td>\n",
       "      <td>86.3066</td>\n",
       "      <td>3060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.8144</td>\n",
       "      <td>94.4981</td>\n",
       "      <td>94.155</td>\n",
       "      <td>5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>72.2272</td>\n",
       "      <td>71.3904</td>\n",
       "      <td>71.8064</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.1552</td>\n",
       "      <td>88.9259</td>\n",
       "      <td>88.5389</td>\n",
       "      <td>2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>95.9781</td>\n",
       "      <td>89.8973</td>\n",
       "      <td>92.8382</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.228</td>\n",
       "      <td>99.3939</td>\n",
       "      <td>97.7853</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>96.793</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.5116</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.951</td>\n",
       "      <td>95.9322</td>\n",
       "      <td>97.4182</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.0446</td>\n",
       "      <td>93.5974</td>\n",
       "      <td>93.3202</td>\n",
       "      <td>1418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>77.8313</td>\n",
       "      <td>80.5989</td>\n",
       "      <td>79.1909</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>90.2093</td>\n",
       "      <td>94.6846</td>\n",
       "      <td>92.3928</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>91.0141</td>\n",
       "      <td>77.4863</td>\n",
       "      <td>83.7072</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>94.4056</td>\n",
       "      <td>65.3753</td>\n",
       "      <td>77.2532</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>92.4855</td>\n",
       "      <td>90.9091</td>\n",
       "      <td>91.6905</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>91.7431</td>\n",
       "      <td>76.3359</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.0159</td>\n",
       "      <td>92.7215</td>\n",
       "      <td>92.8685</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>79.1667</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>76.7677</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>60.2837</td>\n",
       "      <td>82.5243</td>\n",
       "      <td>69.6721</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>58.1006</td>\n",
       "      <td>58.427</td>\n",
       "      <td>58.2633</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>100</td>\n",
       "      <td>4.41176</td>\n",
       "      <td>8.4507</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>82.1086</td>\n",
       "      <td>91.4591</td>\n",
       "      <td>86.532</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.6552</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>89.3805</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>93.9535</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>73.3696</td>\n",
       "      <td>96.4286</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.45</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1    99.891  99.5115  99.7009          3667\n",
       "1                2      93.8  93.9137  93.8568          7746\n",
       "2                3   90.8744  94.3395  92.5745         15933\n",
       "3                4   99.9689  99.3809   99.674         12842\n",
       "4                5   83.5443  98.5075   90.411            66\n",
       "5                6   96.8553  88.5057  92.4925           462\n",
       "6                7   98.0507  96.7773  97.4098          2012\n",
       "7                8   63.1737  50.8434  56.3418           211\n",
       "8                9   63.5838  59.7826  61.6246           220\n",
       "9               10   59.2453  37.4255  45.8729           314\n",
       "10              11   89.3617  97.6744  93.3333            84\n",
       "11              12   92.2353  97.8777  94.9727           784\n",
       "12              13   87.5036  85.1419  86.3066          3060\n",
       "13              14   93.8144  94.4981   94.155          5187\n",
       "14              15   72.2272  71.3904  71.8064           801\n",
       "15              16   88.1552  88.9259  88.5389          2136\n",
       "16              17   95.9781  89.8973  92.8382           525\n",
       "17              18    96.228  99.3939  97.7853          1148\n",
       "18              19    96.793  96.2319  96.5116           332\n",
       "19              20    98.951  95.9322  97.4182           283\n",
       "20              21   93.0446  93.5974  93.3202          1418\n",
       "21              22   77.8313  80.5989  79.1909          1292\n",
       "22              23   90.2093  94.6846  92.3928          1336\n",
       "23              24   91.0141  77.4863  83.7072           709\n",
       "24              25   94.4056  65.3753  77.2532           270\n",
       "25              26   92.4855  90.9091  91.6905           160\n",
       "26              27   91.7431  76.3359  83.3333           100\n",
       "27              29   93.0159  92.7215  92.8685           293\n",
       "28              30   79.1667  74.5098  76.7677            76\n",
       "29              31   60.2837  82.5243  69.6721            85\n",
       "30              32   58.1006   58.427  58.2633           104\n",
       "31              33       100  4.41176   8.4507             3\n",
       "32              34   82.1086  91.4591   86.532           514\n",
       "33              35   55.5556  55.5556  55.5556             5\n",
       "34              36       100    81.25  89.6552            13\n",
       "35              37   89.3805  99.0196  93.9535           101\n",
       "36              38   48.7179  48.7179  48.7179            19\n",
       "37              39   73.3696  96.4286  83.3333           135\n",
       "38              40       100      100      100           280\n",
       "39              41        75       75       75            15\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.45                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12950 samples, validate on 5550 samples\n",
      "Epoch 1/2\n",
      "12950/12950 [==============================] - 66s - loss: 45.1413 - acc: 0.9705 - val_loss: 42.9347 - val_acc: 0.9363\n",
      "Epoch 2/2\n",
      "12950/12950 [==============================] - 66s - loss: 45.1379 - acc: 0.9717 - val_loss: 42.9397 - val_acc: 0.9360\n",
      "CPU times: user 5min 54s, sys: 52.4 s, total: 6min 47s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=2,verbose=1,shuffle=True,validation_split=0.3, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8641</td>\n",
       "      <td>99.6744</td>\n",
       "      <td>99.7691</td>\n",
       "      <td>3673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>93.6979</td>\n",
       "      <td>94.0955</td>\n",
       "      <td>93.8963</td>\n",
       "      <td>7761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90.9699</td>\n",
       "      <td>94.1856</td>\n",
       "      <td>92.5498</td>\n",
       "      <td>15907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.93</td>\n",
       "      <td>99.4428</td>\n",
       "      <td>99.6858</td>\n",
       "      <td>12850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>87.1429</td>\n",
       "      <td>91.0448</td>\n",
       "      <td>89.0511</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>97.4522</td>\n",
       "      <td>87.931</td>\n",
       "      <td>92.4471</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.9116</td>\n",
       "      <td>96.9697</td>\n",
       "      <td>97.4384</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>61.5591</td>\n",
       "      <td>55.1807</td>\n",
       "      <td>58.1957</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>59.6059</td>\n",
       "      <td>65.7609</td>\n",
       "      <td>62.5323</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>61.2903</td>\n",
       "      <td>38.4982</td>\n",
       "      <td>47.2914</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>90.2174</td>\n",
       "      <td>96.5116</td>\n",
       "      <td>93.2584</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>93.599</td>\n",
       "      <td>96.7541</td>\n",
       "      <td>95.1504</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>87.2582</td>\n",
       "      <td>85.3645</td>\n",
       "      <td>86.301</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.3207</td>\n",
       "      <td>94.0973</td>\n",
       "      <td>94.2088</td>\n",
       "      <td>5165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>67.0455</td>\n",
       "      <td>73.6185</td>\n",
       "      <td>70.1784</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.5509</td>\n",
       "      <td>89.592</td>\n",
       "      <td>88.5597</td>\n",
       "      <td>2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.9008</td>\n",
       "      <td>87.8425</td>\n",
       "      <td>92.5993</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.3805</td>\n",
       "      <td>99.1342</td>\n",
       "      <td>97.7379</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>96.4706</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>95.7664</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.9286</td>\n",
       "      <td>93.8983</td>\n",
       "      <td>96.3478</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>93.3245</td>\n",
       "      <td>93.2013</td>\n",
       "      <td>93.2629</td>\n",
       "      <td>1412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>81.1271</td>\n",
       "      <td>77.2302</td>\n",
       "      <td>79.1307</td>\n",
       "      <td>1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.1076</td>\n",
       "      <td>96.2438</td>\n",
       "      <td>92.5383</td>\n",
       "      <td>1358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>90.736</td>\n",
       "      <td>78.1421</td>\n",
       "      <td>83.9695</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>92.0962</td>\n",
       "      <td>64.891</td>\n",
       "      <td>76.1364</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>95.8824</td>\n",
       "      <td>92.6136</td>\n",
       "      <td>94.2197</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>94.2308</td>\n",
       "      <td>74.8092</td>\n",
       "      <td>83.4043</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.5185</td>\n",
       "      <td>95.8861</td>\n",
       "      <td>94.6875</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>76.7677</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>75.6219</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>56.2914</td>\n",
       "      <td>82.5243</td>\n",
       "      <td>66.9291</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>64.3836</td>\n",
       "      <td>52.809</td>\n",
       "      <td>58.0247</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>100</td>\n",
       "      <td>4.41176</td>\n",
       "      <td>8.4507</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>85.8108</td>\n",
       "      <td>90.3915</td>\n",
       "      <td>88.0416</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.6552</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.9091</td>\n",
       "      <td>98.0392</td>\n",
       "      <td>94.3396</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>52.7778</td>\n",
       "      <td>48.7179</td>\n",
       "      <td>50.6667</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>74.0331</td>\n",
       "      <td>95.7143</td>\n",
       "      <td>83.4891</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>76.1905</td>\n",
       "      <td>80</td>\n",
       "      <td>78.0488</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.45</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8641  99.6744  99.7691          3673\n",
       "1                2   93.6979  94.0955  93.8963          7761\n",
       "2                3   90.9699  94.1856  92.5498         15907\n",
       "3                4     99.93  99.4428  99.6858         12850\n",
       "4                5   87.1429  91.0448  89.0511            61\n",
       "5                6   97.4522   87.931  92.4471           459\n",
       "6                7   97.9116  96.9697  97.4384          2016\n",
       "7                8   61.5591  55.1807  58.1957           229\n",
       "8                9   59.6059  65.7609  62.5323           242\n",
       "9               10   61.2903  38.4982  47.2914           323\n",
       "10              11   90.2174  96.5116  93.2584            83\n",
       "11              12    93.599  96.7541  95.1504           775\n",
       "12              13   87.2582  85.3645   86.301          3068\n",
       "13              14   94.3207  94.0973  94.2088          5165\n",
       "14              15   67.0455  73.6185  70.1784           826\n",
       "15              16   87.5509   89.592  88.5597          2152\n",
       "16              17   97.9008  87.8425  92.5993           513\n",
       "17              18   96.3805  99.1342  97.7379          1145\n",
       "18              19   96.4706  95.0725  95.7664           328\n",
       "19              20   98.9286  93.8983  96.3478           277\n",
       "20              21   93.3245  93.2013  93.2629          1412\n",
       "21              22   81.1271  77.2302  79.1307          1238\n",
       "22              23   89.1076  96.2438  92.5383          1358\n",
       "23              24    90.736  78.1421  83.9695           715\n",
       "24              25   92.0962   64.891  76.1364           268\n",
       "25              26   95.8824  92.6136  94.2197           163\n",
       "26              27   94.2308  74.8092  83.4043            98\n",
       "27              29   93.5185  95.8861  94.6875           303\n",
       "28              30   76.7677  74.5098  75.6219            76\n",
       "29              31   56.2914  82.5243  66.9291            85\n",
       "30              32   64.3836   52.809  58.0247            94\n",
       "31              33       100  4.41176   8.4507             3\n",
       "32              34   85.8108  90.3915  88.0416           508\n",
       "33              35   55.5556  55.5556  55.5556             5\n",
       "34              36       100    81.25  89.6552            13\n",
       "35              37   90.9091  98.0392  94.3396           100\n",
       "36              38   52.7778  48.7179  50.6667            19\n",
       "37              39   74.0331  95.7143  83.4891           134\n",
       "38              40       100      100      100           280\n",
       "39              41   76.1905       80  78.0488            16\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.45                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 CRF Marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 5\n",
    "\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train the model</b> from scratch but use the pretrained weight from previous CRF Viterbi model.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 102, 48)           4752      \n",
      "=================================================================\n",
      "Total params: 500,960\n",
      "Trainable params: 500,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation = 'relu')))\n",
    "crf = CRF(n_classes,\n",
    "          learn_mode='marginal',\n",
    "          test_mode='marginal',\n",
    "          sparse_target=False,\n",
    "          use_boundary=True,\n",
    "          use_bias=True,\n",
    "          activation='linear',\n",
    "          kernel_initializer='glorot_uniform',\n",
    "          chain_initializer='orthogonal',\n",
    "          bias_initializer='zeros',\n",
    "          boundary_initializer='zeros',\n",
    "          kernel_regularizer=None,\n",
    "          chain_regularizer=None,\n",
    "          boundary_regularizer=None,\n",
    "          bias_regularizer=None,\n",
    "          kernel_constraint=None,\n",
    "          chain_constraint=None,\n",
    "          boundary_constraint=None,\n",
    "          bias_constraint=None,\n",
    "          input_dim=None,\n",
    "          unroll=False)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.0015)\n",
    "model.compile(optimizer=adam,loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12950 samples, validate on 5550 samples\n",
      "Epoch 1/5\n",
      "12950/12950 [==============================] - 41s - loss: 3.0994 - acc: 0.2890 - val_loss: 2.3322 - val_acc: 0.3539\n",
      "Epoch 2/5\n",
      "12950/12950 [==============================] - 41s - loss: 1.7014 - acc: 0.5549 - val_loss: 1.1082 - val_acc: 0.7399\n",
      "Epoch 3/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.6938 - acc: 0.8325 - val_loss: 0.5602 - val_acc: 0.8738\n",
      "Epoch 4/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.3783 - acc: 0.9089 - val_loss: 0.4497 - val_acc: 0.8864\n",
      "Epoch 5/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.2823 - acc: 0.9311 - val_loss: 0.4058 - val_acc: 0.8936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79087c89b0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=256,epochs=5,verbose=1,shuffle=True,validation_split=0.3, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.7627</td>\n",
       "      <td>91.2585</td>\n",
       "      <td>92.9776</td>\n",
       "      <td>7527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>88.502</td>\n",
       "      <td>96.1632</td>\n",
       "      <td>92.1737</td>\n",
       "      <td>16241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9766</td>\n",
       "      <td>99.3964</td>\n",
       "      <td>99.6857</td>\n",
       "      <td>12844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.082</td>\n",
       "      <td>86.5672</td>\n",
       "      <td>90.625</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.1266</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.6531</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>96.5567</td>\n",
       "      <td>97.114</td>\n",
       "      <td>96.8345</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>78.1818</td>\n",
       "      <td>31.0843</td>\n",
       "      <td>44.4828</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>72.4771</td>\n",
       "      <td>21.4674</td>\n",
       "      <td>33.1237</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>54.159</td>\n",
       "      <td>34.9225</td>\n",
       "      <td>42.4638</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>91.7647</td>\n",
       "      <td>90.6977</td>\n",
       "      <td>91.2281</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>92.8307</td>\n",
       "      <td>93.7578</td>\n",
       "      <td>93.2919</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>80.5578</td>\n",
       "      <td>84.3907</td>\n",
       "      <td>82.4297</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.6674</td>\n",
       "      <td>91.5285</td>\n",
       "      <td>93.0715</td>\n",
       "      <td>5024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>71.7414</td>\n",
       "      <td>61.3191</td>\n",
       "      <td>66.1221</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>82.7175</td>\n",
       "      <td>89.4671</td>\n",
       "      <td>85.96</td>\n",
       "      <td>2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.0573</td>\n",
       "      <td>91.7808</td>\n",
       "      <td>93.8704</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.4527</td>\n",
       "      <td>98.8745</td>\n",
       "      <td>97.6486</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.9104</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>96.4706</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.4446</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>90.9333</td>\n",
       "      <td>90.033</td>\n",
       "      <td>90.4809</td>\n",
       "      <td>1364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>74.1772</td>\n",
       "      <td>73.1129</td>\n",
       "      <td>73.6412</td>\n",
       "      <td>1172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>83.4155</td>\n",
       "      <td>95.8894</td>\n",
       "      <td>89.2186</td>\n",
       "      <td>1353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>76.332</td>\n",
       "      <td>81.4208</td>\n",
       "      <td>78.7943</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>79.2727</td>\n",
       "      <td>52.7845</td>\n",
       "      <td>63.3721</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>88</td>\n",
       "      <td>75</td>\n",
       "      <td>80.9816</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>89.1304</td>\n",
       "      <td>62.5954</td>\n",
       "      <td>73.5426</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>94.3396</td>\n",
       "      <td>94.9367</td>\n",
       "      <td>94.6372</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>66.9903</td>\n",
       "      <td>67.6471</td>\n",
       "      <td>67.3171</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>63.6364</td>\n",
       "      <td>81.5534</td>\n",
       "      <td>71.4894</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>90.6087</td>\n",
       "      <td>92.7046</td>\n",
       "      <td>91.6447</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>78.2946</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>87.4459</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>58.3815</td>\n",
       "      <td>72.1429</td>\n",
       "      <td>64.5367</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>99.6441</td>\n",
       "      <td>100</td>\n",
       "      <td>99.8217</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=91.17</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   94.7627  91.2585  92.9776          7527\n",
       "2                3    88.502  96.1632  92.1737         16241\n",
       "3                4   99.9766  99.3964  99.6857         12844\n",
       "4                5    95.082  86.5672   90.625            58\n",
       "5                6   99.1266  86.9732  92.6531           454\n",
       "6                7   96.5567   97.114  96.8345          2019\n",
       "7                8   78.1818  31.0843  44.4828           129\n",
       "8                9   72.4771  21.4674  33.1237            79\n",
       "9               10    54.159  34.9225  42.4638           293\n",
       "10              11   91.7647  90.6977  91.2281            78\n",
       "11              12   92.8307  93.7578  93.2919           751\n",
       "12              13   80.5578  84.3907  82.4297          3033\n",
       "13              14   94.6674  91.5285  93.0715          5024\n",
       "14              15   71.7414  61.3191  66.1221           688\n",
       "15              16   82.7175  89.4671    85.96          2149\n",
       "16              17   96.0573  91.7808  93.8704           536\n",
       "17              18   96.4527  98.8745  97.6486          1142\n",
       "18              19   97.9104  95.0725  96.4706           328\n",
       "19              20   97.9452  96.9492  97.4446           286\n",
       "20              21   90.9333   90.033  90.4809          1364\n",
       "21              22   74.1772  73.1129  73.6412          1172\n",
       "22              23   83.4155  95.8894  89.2186          1353\n",
       "23              24    76.332  81.4208  78.7943           745\n",
       "24              25   79.2727  52.7845  63.3721           218\n",
       "25              26        88       75  80.9816           132\n",
       "26              27   89.1304  62.5954  73.5426            82\n",
       "27              29   94.3396  94.9367  94.6372           300\n",
       "28              30   66.9903  67.6471  67.3171            69\n",
       "29              31   63.6364  81.5534  71.4894            84\n",
       "30              32         -        0        -             0\n",
       "31              33         -        0        -             0\n",
       "32              34   90.6087  92.7046  91.6447           521\n",
       "33              35         -        0        -             0\n",
       "34              36         -        0        -             0\n",
       "35              37   78.2946  99.0196  87.4459           101\n",
       "36              38         -        0        -             0\n",
       "37              39   58.3815  72.1429  64.5367           101\n",
       "38              40   99.6441      100  99.8217           280\n",
       "39              41         -        0        -             0\n",
       "40              42         -        0        -             0\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=91.17                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12950 samples, validate on 5550 samples\n",
      "Epoch 1/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.2387 - acc: 0.9387 - val_loss: 0.3884 - val_acc: 0.8949\n",
      "Epoch 2/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.2132 - acc: 0.9430 - val_loss: 0.3679 - val_acc: 0.9016\n",
      "Epoch 3/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.1974 - acc: 0.9454 - val_loss: 0.3536 - val_acc: 0.9092\n",
      "Epoch 4/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.1845 - acc: 0.9479 - val_loss: 0.3416 - val_acc: 0.9150\n",
      "Epoch 5/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.1751 - acc: 0.9501 - val_loss: 0.3337 - val_acc: 0.9177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79087c8630>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=256,epochs=5,verbose=1,shuffle=True,validation_split=0.3, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>95.3231</td>\n",
       "      <td>91.9253</td>\n",
       "      <td>93.5934</td>\n",
       "      <td>7582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.4036</td>\n",
       "      <td>96.2165</td>\n",
       "      <td>92.685</td>\n",
       "      <td>16250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.6053</td>\n",
       "      <td>99.7868</td>\n",
       "      <td>12871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>89.5522</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.1266</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.6531</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.4818</td>\n",
       "      <td>96.8254</td>\n",
       "      <td>97.1525</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>74.1935</td>\n",
       "      <td>38.7952</td>\n",
       "      <td>50.9494</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>73.1602</td>\n",
       "      <td>45.9239</td>\n",
       "      <td>56.4274</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>64.1618</td>\n",
       "      <td>39.6901</td>\n",
       "      <td>49.0427</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>93.2584</td>\n",
       "      <td>96.5116</td>\n",
       "      <td>94.8571</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>95.0739</td>\n",
       "      <td>96.3795</td>\n",
       "      <td>95.7223</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>84.7453</td>\n",
       "      <td>85.1697</td>\n",
       "      <td>84.957</td>\n",
       "      <td>3061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>94.3965</td>\n",
       "      <td>93.6054</td>\n",
       "      <td>93.9993</td>\n",
       "      <td>5138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>77.5553</td>\n",
       "      <td>65.5971</td>\n",
       "      <td>71.0768</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>86.9337</td>\n",
       "      <td>89.4671</td>\n",
       "      <td>88.1822</td>\n",
       "      <td>2149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.5104</td>\n",
       "      <td>80.4795</td>\n",
       "      <td>88.1801</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.3805</td>\n",
       "      <td>99.1342</td>\n",
       "      <td>97.7379</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.3294</td>\n",
       "      <td>95.0725</td>\n",
       "      <td>96.1877</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.4446</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>92.074</td>\n",
       "      <td>92.0132</td>\n",
       "      <td>92.0436</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>80.6706</td>\n",
       "      <td>76.544</td>\n",
       "      <td>78.5531</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>86.3752</td>\n",
       "      <td>96.5982</td>\n",
       "      <td>91.2011</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>80.6277</td>\n",
       "      <td>81.4208</td>\n",
       "      <td>81.0223</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>86.8167</td>\n",
       "      <td>65.3753</td>\n",
       "      <td>74.5856</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>91.3295</td>\n",
       "      <td>89.7727</td>\n",
       "      <td>90.5444</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>92.7273</td>\n",
       "      <td>77.8626</td>\n",
       "      <td>84.6473</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.5373</td>\n",
       "      <td>98.1013</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>70.4762</td>\n",
       "      <td>72.549</td>\n",
       "      <td>71.4976</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>43.5897</td>\n",
       "      <td>82.5243</td>\n",
       "      <td>57.047</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>78.8618</td>\n",
       "      <td>54.4944</td>\n",
       "      <td>64.4518</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>88.8889</td>\n",
       "      <td>91.1032</td>\n",
       "      <td>89.9824</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>80.8</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>88.9868</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>32.5</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>32.9114</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>69.3182</td>\n",
       "      <td>87.1429</td>\n",
       "      <td>77.2152</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>98.5915</td>\n",
       "      <td>100</td>\n",
       "      <td>99.2908</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.26</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   95.3231  91.9253  93.5934          7582\n",
       "2                3   89.4036  96.2165   92.685         16250\n",
       "3                4   99.9689  99.6053  99.7868         12871\n",
       "4                5   95.2381  89.5522  92.3077            60\n",
       "5                6   99.1266  86.9732  92.6531           454\n",
       "6                7   97.4818  96.8254  97.1525          2013\n",
       "7                8   74.1935  38.7952  50.9494           161\n",
       "8                9   73.1602  45.9239  56.4274           169\n",
       "9               10   64.1618  39.6901  49.0427           333\n",
       "10              11   93.2584  96.5116  94.8571            83\n",
       "11              12   95.0739  96.3795  95.7223           772\n",
       "12              13   84.7453  85.1697   84.957          3061\n",
       "13              14   94.3965  93.6054  93.9993          5138\n",
       "14              15   77.5553  65.5971  71.0768           736\n",
       "15              16   86.9337  89.4671  88.1822          2149\n",
       "16              17   97.5104  80.4795  88.1801           470\n",
       "17              18   96.3805  99.1342  97.7379          1145\n",
       "18              19   97.3294  95.0725  96.1877           328\n",
       "19              20   97.9452  96.9492  97.4446           286\n",
       "20              21    92.074  92.0132  92.0436          1394\n",
       "21              22   80.6706   76.544  78.5531          1227\n",
       "22              23   86.3752  96.5982  91.2011          1363\n",
       "23              24   80.6277  81.4208  81.0223           745\n",
       "24              25   86.8167  65.3753  74.5856           270\n",
       "25              26   91.3295  89.7727  90.5444           158\n",
       "26              27   92.7273  77.8626  84.6473           102\n",
       "27              29   92.5373  98.1013  95.2381           310\n",
       "28              30   70.4762   72.549  71.4976            74\n",
       "29              31   43.5897  82.5243   57.047            85\n",
       "30              32   78.8618  54.4944  64.4518            97\n",
       "31              33         -        0        -             0\n",
       "32              34   88.8889  91.1032  89.9824           512\n",
       "33              35   55.5556  55.5556  55.5556             5\n",
       "34              36         -        0        -             0\n",
       "35              37      80.8  99.0196  88.9868           101\n",
       "36              38      32.5  33.3333  32.9114            13\n",
       "37              39   69.3182  87.1429  77.2152           122\n",
       "38              40   98.5915      100  99.2908           280\n",
       "39              41        70       70       70            14\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.26                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12950 samples, validate on 5550 samples\n",
      "Epoch 1/5\n",
      "12950/12950 [==============================] - 40s - loss: 0.1658 - acc: 0.9518 - val_loss: 0.3321 - val_acc: 0.9176\n",
      "Epoch 2/5\n",
      "12950/12950 [==============================] - 40s - loss: 0.1586 - acc: 0.9538 - val_loss: 0.3263 - val_acc: 0.9195\n",
      "Epoch 3/5\n",
      "12950/12950 [==============================] - 40s - loss: 0.1531 - acc: 0.9547 - val_loss: 0.3230 - val_acc: 0.9201\n",
      "Epoch 4/5\n",
      "12950/12950 [==============================] - 41s - loss: 0.1470 - acc: 0.9562 - val_loss: 0.3201 - val_acc: 0.9211\n",
      "Epoch 5/5\n",
      "12950/12950 [==============================] - 40s - loss: 0.1415 - acc: 0.9577 - val_loss: 0.3207 - val_acc: 0.9224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79071dcbe0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=256,epochs=5,verbose=1,shuffle=True,validation_split=0.3, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8097</td>\n",
       "      <td>99.6472</td>\n",
       "      <td>99.7284</td>\n",
       "      <td>3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.8731</td>\n",
       "      <td>92.8831</td>\n",
       "      <td>93.8675</td>\n",
       "      <td>7661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>89.3545</td>\n",
       "      <td>96.9625</td>\n",
       "      <td>93.0032</td>\n",
       "      <td>16376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.5434</td>\n",
       "      <td>99.7557</td>\n",
       "      <td>12863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>95.2381</td>\n",
       "      <td>89.5522</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7802</td>\n",
       "      <td>86.9732</td>\n",
       "      <td>92.9376</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>96.887</td>\n",
       "      <td>97.3064</td>\n",
       "      <td>97.0962</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>73.5632</td>\n",
       "      <td>46.2651</td>\n",
       "      <td>56.8047</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>75.2941</td>\n",
       "      <td>52.1739</td>\n",
       "      <td>61.6372</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>66.3286</td>\n",
       "      <td>38.975</td>\n",
       "      <td>49.0991</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>87.5</td>\n",
       "      <td>97.6744</td>\n",
       "      <td>92.3077</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>94.813</td>\n",
       "      <td>98.1273</td>\n",
       "      <td>96.4417</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.1588</td>\n",
       "      <td>83.7507</td>\n",
       "      <td>86.3702</td>\n",
       "      <td>3010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>95.2434</td>\n",
       "      <td>93.3868</td>\n",
       "      <td>94.306</td>\n",
       "      <td>5126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>78.9631</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>74.5412</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>87.5557</td>\n",
       "      <td>89.9251</td>\n",
       "      <td>88.7246</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.3196</td>\n",
       "      <td>80.8219</td>\n",
       "      <td>88.3068</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>96.3835</td>\n",
       "      <td>99.2208</td>\n",
       "      <td>97.7816</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.076</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.6521</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.9619</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>92.5099</td>\n",
       "      <td>92.9373</td>\n",
       "      <td>92.7231</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>83.2888</td>\n",
       "      <td>77.7293</td>\n",
       "      <td>80.413</td>\n",
       "      <td>1246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>88.2924</td>\n",
       "      <td>96.7399</td>\n",
       "      <td>92.3233</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>85.4525</td>\n",
       "      <td>81.5301</td>\n",
       "      <td>83.4452</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>88.9251</td>\n",
       "      <td>66.1017</td>\n",
       "      <td>75.8333</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>84.9741</td>\n",
       "      <td>93.1818</td>\n",
       "      <td>88.8889</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>93.1373</td>\n",
       "      <td>72.5191</td>\n",
       "      <td>81.5451</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>93.7888</td>\n",
       "      <td>95.5696</td>\n",
       "      <td>94.6708</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>72.381</td>\n",
       "      <td>74.5098</td>\n",
       "      <td>73.43</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>45.8333</td>\n",
       "      <td>85.4369</td>\n",
       "      <td>59.661</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>63.9456</td>\n",
       "      <td>52.809</td>\n",
       "      <td>57.8462</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>88.676</td>\n",
       "      <td>90.5694</td>\n",
       "      <td>89.6127</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>87.8261</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>93.0876</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>45.4545</td>\n",
       "      <td>38.4615</td>\n",
       "      <td>41.6667</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>73.3766</td>\n",
       "      <td>80.7143</td>\n",
       "      <td>76.8707</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>99.6441</td>\n",
       "      <td>100</td>\n",
       "      <td>99.8217</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>75</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=92.71</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8097  99.6472  99.7284          3672\n",
       "1                2   94.8731  92.8831  93.8675          7661\n",
       "2                3   89.3545  96.9625  93.0032         16376\n",
       "3                4   99.9689  99.5434  99.7557         12863\n",
       "4                5   95.2381  89.5522  92.3077            60\n",
       "5                6   99.7802  86.9732  92.9376           454\n",
       "6                7    96.887  97.3064  97.0962          2023\n",
       "7                8   73.5632  46.2651  56.8047           192\n",
       "8                9   75.2941  52.1739  61.6372           192\n",
       "9               10   66.3286   38.975  49.0991           327\n",
       "10              11      87.5  97.6744  92.3077            84\n",
       "11              12    94.813  98.1273  96.4417           786\n",
       "12              13   89.1588  83.7507  86.3702          3010\n",
       "13              14   95.2434  93.3868   94.306          5126\n",
       "14              15   78.9631  70.5882  74.5412           792\n",
       "15              16   87.5557  89.9251  88.7246          2160\n",
       "16              17   97.3196  80.8219  88.3068           472\n",
       "17              18   96.3835  99.2208  97.7816          1146\n",
       "18              19    97.076  96.2319  96.6521           332\n",
       "19              20   98.9619  96.9492  97.9452           286\n",
       "20              21   92.5099  92.9373  92.7231          1408\n",
       "21              22   83.2888  77.7293   80.413          1246\n",
       "22              23   88.2924  96.7399  92.3233          1365\n",
       "23              24   85.4525  81.5301  83.4452           746\n",
       "24              25   88.9251  66.1017  75.8333           273\n",
       "25              26   84.9741  93.1818  88.8889           164\n",
       "26              27   93.1373  72.5191  81.5451            95\n",
       "27              29   93.7888  95.5696  94.6708           302\n",
       "28              30    72.381  74.5098    73.43            76\n",
       "29              31   45.8333  85.4369   59.661            88\n",
       "30              32   63.9456   52.809  57.8462            94\n",
       "31              33         -        0        -             0\n",
       "32              34    88.676  90.5694  89.6127           509\n",
       "33              35   55.5556  55.5556  55.5556             5\n",
       "34              36       100     87.5  93.3333            14\n",
       "35              37   87.8261  99.0196  93.0876           101\n",
       "36              38   45.4545  38.4615  41.6667            15\n",
       "37              39   73.3766  80.7143  76.8707           113\n",
       "38              40   99.6441      100  99.8217           280\n",
       "39              41   71.4286       75  73.1707            15\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=92.71                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('/data/marginal_crf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 6\n",
    "\n",
    "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (which model perform better, why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>\n",
    "<pre style=\"white-space: wrap; padding-right: 10px\">\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The result from marginal CRF and viterbi CRF yield similar result. But as seen above the best result is from viterbi CRF with 92.87 accuracy. When compared to marginal probabilities, with 92.71 accuracy, which yield very little difference. `Forward-Backward gives <i>marginal</i> probability for each individual state, <i>Viterbi</i> gives probability of the most likely sequence of states` (<a href=\"https://stats.stackexchange.com/questions/31746/what-is-the-difference-between-the-forward-backward-and-viterbi-algorithms/222270\">reference</a>). Viterbi will give the best result from a given choices while marginal will calculate from inputs to outputs. Viterbi is dynamic programming algorithm, but marginal is greedy algorithm as its core. Because the difference in their nature, DP give best results while greedy give good enough result. Viterbi is prefered if the state is small but marginal will give faster result.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
