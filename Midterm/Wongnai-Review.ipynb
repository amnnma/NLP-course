{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_csv = '~/Midterm/data/w_review_train.csv'\n",
    "sep = ';'\n",
    "header = None\n",
    "\n",
    "df = pd.read_csv(trained_csv, sep=sep, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wordX, wordY = df[0], df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import nltk\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekapolc/.env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Embedding, Conv1D, TimeDistributed, Activation, Dense, Flatten, GRU, Bidirectional, Dropout, LSTM, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokenizer_model():\n",
    "    input1 = Input(shape=(21,))\n",
    "    x = Embedding(178,8)(input1)\n",
    "    x = Conv1D(100,5,strides=1,activation='relu',padding=\"same\")(x)\n",
    "    x = TimeDistributed(Dense(5))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 21, 8)             1424      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 21, 100)           4100      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 5)             505       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 105)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 36,930\n",
      "Trainable params: 36,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer_model = get_tokenizer_model()\n",
    "tokenizer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_path_model_best='/data/model_best_2.h5'\n",
    "tokenizer_model.load_weights(tokenizer_path_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "  n_pad_2 = int((n_pad - 1)/2)\n",
    "  for i in range(n_pad_2):\n",
    "      df['char-{}'.format(i+1)] = df[0].shift(i + 1)\n",
    "      df['char{}'.format(i+1)] = df[0].shift(-i - 1)\n",
    "  return df[n_pad_2: -n_pad_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, model):\n",
    "    character = list(sentence)\n",
    "    character_df = pd.DataFrame(character)\n",
    "\n",
    "    n_pad = 21\n",
    "    n_pad_2 = int((n_pad - 1)/2)\n",
    "    pad = [{0: ' '}]\n",
    "    df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "    character_df = pd.concat((df_pad, character_df, df_pad))\n",
    "    character_df[0] = character_df[0].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "    \n",
    "    df_with_context = create_n_gram_df(character_df, n_pad=n_pad)\n",
    "\n",
    "    char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "                 ['char-' + str(i + 1) for i in range(n_pad_2)] + [0]\n",
    "\n",
    "    x_char = df_with_context[char_row].as_matrix()\n",
    "    y_pred = model.predict(x_char)\n",
    "\n",
    "    prob_to_class = lambda p: 1 if p[0]>=0.5 else 0\n",
    "    y_pred = np.apply_along_axis(prob_to_class,1,y_pred)\n",
    "    \n",
    "    tokenize = []\n",
    "    accumulator = ''\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == 1):\n",
    "            tokenize.append(accumulator)\n",
    "            accumulator = character[i]\n",
    "        else:\n",
    "            accumulator += character[i]\n",
    "    else:\n",
    "        tokenize.append(accumulator)\n",
    "    tokenize = tokenize[1:]\n",
    "    return tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_threshold = 3\n",
    "\n",
    "def tokenize_corpus(df, threshold = word_threshold):\n",
    "    num_UNK = 0\n",
    "    rare_word = set()\n",
    "    corpus = set()\n",
    "    word_freq = defaultdict(lambda: 0)\n",
    "    corpus_list = []\n",
    "    \n",
    "    for idx in tqdm(range(len(df))):\n",
    "        target = df[idx]\n",
    "        target = re.sub(r'[$|!|~|\\n|-|#|+|<|>|0-9|*|.|%|@|$|^|&|=|:|:D|(|)|-|_|\\t|?|;|\\/|\\\\|\\||{|}|\\\\]|\\\\[|`]|\"|๑|๒|๓|๔|๕|๖|๗|๘|๙|๐|a-z|A-Z|\\'', r'', target)\n",
    "        tokenize_list = list(map(lambda x: re.sub('\\s+','', x), list(filter(lambda x: len(x.strip()) \\\n",
    "                    != 0 and x not in conjunction_word, tokenize(target, my_best_model)))))\n",
    "        corpus_list.append(tokenize_list)\n",
    "        for x in tokenize_list:\n",
    "            word_freq[x] += 1\n",
    "\n",
    "    for x in word_freq:\n",
    "        if(word_freq[x] <= threshold):\n",
    "            num_UNK += word_freq[x]\n",
    "            rare_word.add(x)\n",
    "        else:\n",
    "            corpus.add(x)\n",
    "    \n",
    "    print(\"Approximate {} % are normal words ({} of {})\".format(100*len(corpus)/(len(corpus) + len(rare_word)),\\\n",
    "                                                                len(corpus), len(rare_word) + len(corpus)))\n",
    "    return list(map(lambda x: list(filter(lambda y: y in corpus, x)), corpus_list)), corpus, rare_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "#Threshold processing\n",
    "def process_threshold(lis, threshold = word_threshold):\n",
    "    word_freq = defaultdict(lambda: 0)\n",
    "    num_UNK = 0\n",
    "    rare_word = set()\n",
    "    word_freq = defaultdict(lambda: 0)\n",
    "    \n",
    "    for x in lis:\n",
    "        for word in x:\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    for x in word_freq:\n",
    "        if(word_freq[x] <= threshold):\n",
    "            num_UNK += word_freq[x]\n",
    "            rare_word.add(x)\n",
    "\n",
    "    word_freq[\"UNK\"] = num_UNK\n",
    "    \n",
    "    def abc(word):\n",
    "        if word not in rare_word:\n",
    "            return word\n",
    "        return \"UNK\"\n",
    "    \n",
    "    return list(map(lambda x: list(map(lambda y: abc(y), x)), lis)), word_freq, rare_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# content_list_0 = tokenize_corpus(wordX, threshold=0)\n",
    "# save_object(content_list_0, '/data/tokenizeWordAll-2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_list_0 = load_object('/data/tokenizeWordAll-2.pkl')\n",
    "content_list_0 = process_threshold(content_list_0[0], threshold = word_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i+1\n",
    "\n",
    "fileSave = \"/data/personalWord2Vec.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec = dict()\n",
    "# filePath = \"/data/fastText/thai2vec.vec\"\n",
    "# fileLen = file_len(filePath)\n",
    "# with open(filePath, 'r') as f:\n",
    "#     for idx, line in tqdm(enumerate(f), total = fileLen):\n",
    "#         if idx == 0:\n",
    "#             continue\n",
    "#         word2vec[line.split(' ')[0]] = [float(x) for x in line.split(' ')[1:]]\n",
    "# save_object(word2vec, fileSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = load_object(fileSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index = dict()\n",
    "index_to_word = dict()\n",
    "for idx, x in enumerate(word2vec.keys()):\n",
    "    word_to_index[x] = idx\n",
    "    index_to_word[idx] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ร้าน', 'อาหาร', 'ใหญ่', 'มากกกกกกก', 'เลี้ยว', 'เข้า', 'มา', 'เจอ', 'ห้อง', 'น้ำ', 'ก่อน', 'เลย', 'เออ', 'แปลก', 'ดี', 'UNK', 'หลัก', 'ๆ', 'อยู่', 'ชั้น', 'มี', 'กาแฟ', 'น้ำ', 'ผึ้ง', 'ซึ่ง', 'ก็', 'แค่', 'เอา', 'น้ำ', 'ผึ้ง', 'มา', 'ราด', 'แพงเวอร์', 'อย่า', 'สั่ง', 'เลย', 'ลาบ', 'ไข่', 'ต้ม', 'ไข่', 'มัน', 'คาว', 'อะ', 'เลย', 'ไม่', 'ประทับใจ', 'เท่า', 'ไหร่', 'ทอด', 'มัน', 'หัว', 'UNK', 'อร่อย', 'ต้อง', 'เบิ้ล', 'พะแนง', 'ห่อ', 'ไข่', 'อร่อย', 'ดี', 'เห้ย', 'แต่', 'ราคา', 'บาท', 'มัน', 'เกิน', 'ไป', 'นะ', 'รับ', 'ไม่', 'UNK', 'เลิก', 'กิน', 'แล้ว', 'มี', 'ขนม', 'หวาน', 'ให้', 'กิน', 'ฟรี', 'เล็กน้อย', 'ขนม', 'ไทย', 'คง', 'ไม่', 'ไปซ้ำ', 'แพงเกิน'], ['อาหาร', 'ที่', 'นี่', 'เป็น', 'อาหารจีน', 'แคะ', 'ที่', 'หากิน', 'ยาก', 'ใน', 'บ้าน', 'เรา', 'ตัว', 'ร้าน', 'ตั้ง', 'อยู่', 'ที่', 'ถนนพุทธมณฑล', 'สาย', 'ไป', 'ตาม', 'UNK', '-', 'นครชัยศรี', 'เมื่อ', 'ถึง', 'พุทธมณฑลสาย', 'ก็', 'เลี้ยว', 'เข้า', 'ไป', 'ประมาณ', 'เมตร', 'ร้าน', 'อยู่', 'ทาง', 'ซ้าย', 'มือ', 'ค่ะ', 'มี', 'คน', 'บอก', 'มา', 'ว่า', 'ความ', 'พิเศษ', 'ของ', 'ร้าน', 'นี้', 'คือ', 'กุ๊ก', 'เก่า', 'และ', 'เป็น', 'UNK', 'สุดท้าย', 'จาก', 'ฮก', 'ลก', 'UNK', '”', 'ภัตตาคารจีน', 'ชื่อ', 'ดัง', 'ย่าน', 'ราชประสงค์', 'ที่', 'เลิก', 'กิจการ', 'ไป', 'แล้ว', 'ต้อง', 'คน', 'ที่', 'อายุ', 'เลข', 'ขึ้น', 'ไป', 'จึง', 'จะ', 'เคย', 'กิน', 'ฮก', 'ลก', 'UNK', 'จาน', 'เด็ด', 'ที่', 'มี', 'ขาย', 'ที่', 'นี่', 'แห่ง', 'เดียว', 'ใน', 'เมือง', 'ไทย', 'คือ', 'ปลา', 'เต๋าเต้ย', 'ฤดู', 'เป็น', 'สูตร', 'จาก', 'มาเลเซีย', 'นอก', 'นั้น', 'ก็', 'มี', 'ผัด', 'ผัก', 'น้ำมัน', 'หอย', 'ไก่', 'เบตง', 'เคา', 'หยก', 'ปู', 'ทะเล', 'ซุป', 'น้ำ', 'ใส', 'หม้อ', 'ไฟ', 'เต้าหู้', 'แคระ', 'ยัด', 'ไส้', 'หม้อ', 'ดิน', 'และ', 'ลูก', 'ชิ้น', 'แคระ', 'อาหาร', 'ที่', 'เรา', 'แนะนำ', 'คือ', 'ไก่', 'เบตง', 'คล้าย', 'ๆ', 'ไก่', 'แช่', 'เหล้า', 'เสริฟ', 'พร้อม', 'กับ', 'หอม', 'เจียว', 'และ', 'น้ำ', 'จิ้ม', 'น้ำพริก', 'เผา', 'สูตร', 'เด็ด', 'ของ', 'ทาง', 'ร้าน', 'เมนู', 'ข้าว', 'ผัด', 'หนำ', 'เลียบ', 'ก็', 'อร่อย', 'ค่ะ', 'ชอบ', 'มาก', 'ๆ']]\n"
     ]
    }
   ],
   "source": [
    "#Prepare word embedding for unknown word\n",
    "\n",
    "word_freq = content_list_0[1]\n",
    "\n",
    "dictionary = dict()\n",
    "dictionary_r = dict()\n",
    "\n",
    "dictionary[\"for_keras_zero_padding\"] = 0\n",
    "dictionary_r[0] = \"for_keras_zero_padding\"\n",
    "\n",
    "rare_word = set()\n",
    "\n",
    "for word in word_freq:\n",
    "    c += 1\n",
    "    if c <= 10:\n",
    "        print(word, end='|')\n",
    "    t = len(dictionary)\n",
    "    dictionary[word] = t\n",
    "    dictionary_r[t] = word\n",
    "    \n",
    "    if word_freq[word] <= word_threshold:\n",
    "        rare_word.add(word)\n",
    "\n",
    "data = list()\n",
    "data_feature = []\n",
    "\n",
    "for sentence in content_list_0[0]:\n",
    "    for word in sentence:\n",
    "        if word not in rare_word:\n",
    "            data.append(dictionary[word])\n",
    "        else:\n",
    "            data.append(dictionary[\"UNK\"])\n",
    "    data_feature.append(data)\n",
    "    data =[]\n",
    "\n",
    "print(list(map(lambda lis: list(map(lambda y: dictionary_r[y], lis)), data_feature[:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create embedding Main train_data\n",
    "pre_embedding = []\n",
    "pre_embedding.append(np.zeros(300))\n",
    "for l in dictionary.keys():\n",
    "    if l in word_to_index:\n",
    "        pre_embedding.append(word2vec[l])\n",
    "    else:\n",
    "        pre_embedding.append(np.zeros(300)) \n",
    "weight_em = np.array(pre_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119458, 300)\n"
     ]
    }
   ],
   "source": [
    "print(weight_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1000)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(data_feature, maxlen=max_dim, padding='post', truncating='pre')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_review_file_path = '~/Midterm/data/test_file.csv'\n",
    "sep = ';'\n",
    "header = None\n",
    "\n",
    "df_test = pd.read_csv(test_review_file_path, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTokenizeLocation = \"/data/testTokenize.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize_test, kwn_wds, unk_wds = tokenize_corpus(df_test[\"review\"], 0)\n",
    "# tokenize_test, word_freq_test, unk_wds = process_threshold(tokenize_test, threshold = word_threshold)\n",
    "# save_object(tokenize_test, testTokenizeLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_review = load_object(testTokenizeLocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6203, 1000)\n",
      "['ร้าน', 'นี้', 'จะ', 'อยู่', 'เส้น', 'สัน', 'กำแพง', '-', 'แม่ออน', 'เลย', 'แยก', 'บ่อ', 'สร้าง', 'ร้าน', 'จะ', 'อยู่', 'ด้าน', 'ซ้าย', 'ติด', 'ริม', 'ถนน', 'มี', 'ป้าย', 'ติด', 'ไว้', 'เห็น', 'ชัดเจน', 'ปู', 'ทอง', 'UNK', 'เด็ด', 'ตาม', 'หา', 'ข้าว', 'แกงรสชาติ', 'นี้', 'มา', 'ตลอด', 'ใน', 'ที่สุด', 'ก็', 'ได้', 'เจอ', 'เพราะ', 'ส่วน', 'ใหญ่', 'จะ', 'เจอรสชาติ', 'กลาง', 'ๆ', 'ถ้า', 'คน', 'ชอบรส', 'จัด', 'จ้าน', 'แต่', 'ไม่', 'ถึง', 'กับ', 'เผ็ด', 'เว่อร์', 'แนะนำ', 'ร้าน', 'นี้', 'เลย', 'ค่ะ', 'ที่', 'นั่ง', 'ก็', 'สะอาดสะอ้าน', 'มี', 'ทั้ง', 'น้ำ', 'ซุป', 'และ', 'น้ำ', 'พริก', 'กะปิฟรี', 'และ', 'น้ำ', 'ดื่ม', 'ฟรีบริการ', 'ตัว', 'เอง', 'คหสต', 'ชอบแบบ', 'นี้', 'ที่', 'ไม่', 'บังคับ', 'ให้', 'ลูกค้า', 'ต้อง', 'ซื้อ', 'น้ำ', 'กิน', 'เพราะ', 'ทาน', 'คน', 'เดียว', 'แค่', 'ขวด', 'เหลือ', 'ทิ้ง', 'ตลอด', 'เสียดาย', 'เงิน', 'สั่ง', 'กับ', 'ข้าว', 'ราด', 'มา', 'อย่าง', 'อร่อย', 'ทั้ง', 'สอง', 'อย่าง', 'เสียดาย', 'ไข่', 'ดาว', 'หมด', 'ซะก่อน', 'แต่', 'แค่', 'อย่าง', 'กับ', 'ข้าว', 'สวย', 'ที่', 'ให้', 'มา', 'ใน', 'ปริมาณ', 'ที่', 'อิ่ม', 'พอดี', 'ก็', 'เต็มที่', 'แล้ว', 'แม่', 'ค้า', 'ก็', 'อัธยา', 'ศัยดี', 'รับรอง', 'ว่า', 'มี', 'กลับ', 'มา', 'กิน', 'อีก', 'บ่อย', 'ๆ', 'แน่นอน', 'ใคร', 'ผ่าน', 'มา', 'แถว', 'นี้', 'ลอง', 'แวะ', 'ชิมนะ', 'คะ', 'สำหรับ', 'คน', 'ที่', 'UNK', 'กล่อม', 'ไม่', 'จืด', 'ชืด', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding', 'for_keras_zero_padding']\n"
     ]
    }
   ],
   "source": [
    "#Make input feature for test data\n",
    "data =[]\n",
    "test_feature = []\n",
    "for x in test_review:\n",
    "    for word in x:\n",
    "        if word in dictionary.keys():\n",
    "            data.append(dictionary[word])\n",
    "        else:\n",
    "            data.append(dictionary[\"UNK\"])\n",
    "    test_feature.append(data)\n",
    "    data =[]\n",
    "test =[]\n",
    "test = sequence.pad_sequences(test_feature, maxlen=max_dim, padding='post', truncating='pre')\n",
    "print(test.shape)\n",
    "print(list(map(lambda x: dictionary_r[x], test[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Prepare data for training\n",
    "input_data = x_train\n",
    "splitter_idx = int(len(input_data)*0.8)\n",
    "train_data = input_data[:splitter_idx]\n",
    "val_data = input_data[splitter_idx:]\n",
    "\n",
    "label = list(df[1])\n",
    "y_train = pd.get_dummies(pd.Series(label)).as_matrix()\n",
    "\n",
    "target = y_train\n",
    "train_target = target[:splitter_idx]\n",
    "val_target = target[splitter_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_my_best_model():\n",
    "    input1 = Input(shape=(max_dim,))\n",
    "    x = Embedding(weight_em.shape[0],300, weights = [weight_em])(input1)\n",
    "    x = Conv1D(64,10,strides=2,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1,padding='valid')(x)\n",
    "    x = Dense(100)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(100)(x)\n",
    "    x = Conv1D(32,5,strides=2,activation='relu',padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(pool_size=5, strides=1,padding='valid')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = TimeDistributed(Dense(10))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 300)         35837400  \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 496, 64)           192064    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 492, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 492, 100)          6500      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 492, 100)          0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 492, 100)          10100     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 244, 32)           16032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 240, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 240, 32)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 240, 10)           330       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 100)               240100    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 36,313,131\n",
      "Trainable params: 36,313,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_my_best_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path_model='/data/WongNai_Review_New_3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.2374 - categorical_accuracy: 0.4585Epoch 00000: val_loss improved from inf to 1.18946, saving model to /data/WongNai_Review_New_3.h5\n",
      "32000/32000 [==============================] - 37s - loss: 1.2372 - categorical_accuracy: 0.4584 - val_loss: 1.1895 - val_categorical_accuracy: 0.4691\n",
      "Epoch 2/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 1.1161 - categorical_accuracy: 0.4931Epoch 00001: val_loss improved from 1.18946 to 1.01502, saving model to /data/WongNai_Review_New_3.h5\n",
      "32000/32000 [==============================] - 38s - loss: 1.1157 - categorical_accuracy: 0.4935 - val_loss: 1.0150 - val_categorical_accuracy: 0.5323\n",
      "Epoch 3/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.9341 - categorical_accuracy: 0.5811Epoch 00002: val_loss did not improve\n",
      "32000/32000 [==============================] - 36s - loss: 0.9337 - categorical_accuracy: 0.5814 - val_loss: 1.0197 - val_categorical_accuracy: 0.5319\n",
      "Epoch 4/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.7409 - categorical_accuracy: 0.6786Epoch 00003: val_loss did not improve\n",
      "32000/32000 [==============================] - 37s - loss: 0.7409 - categorical_accuracy: 0.6786 - val_loss: 1.1572 - val_categorical_accuracy: 0.5265\n",
      "Epoch 5/5\n",
      "31744/32000 [============================>.] - ETA: 0s - loss: 0.4692 - categorical_accuracy: 0.8129Epoch 00004: val_loss did not improve\n",
      "32000/32000 [==============================] - 37s - loss: 0.4700 - categorical_accuracy: 0.8125 - val_loss: 1.6118 - val_categorical_accuracy: 0.4641\n",
      "CPU times: user 1min 38s, sys: 44.5 s, total: 2min 23s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "callbacks_list_model = [\n",
    "    ModelCheckpoint(\n",
    "        weight_path_model,\n",
    "        monitor = \"val_loss\",\n",
    "        mode = 'min',\n",
    "        verbose = 1,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "    )   \n",
    "]\n",
    "\n",
    "model.fit(train_data,train_target,\n",
    "          batch_size=256,epochs=5,\n",
    "          verbose=1,callbacks=callbacks_list_model,\n",
    "          validation_data=(val_data, val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_path_model)\n",
    "y_pred = model.predict(test)\n",
    "\n",
    "ans = np.argmax(y_pred, axis=1)\n",
    "ans = [x+1 for x in ans]\n",
    "d =dict()\n",
    "d['reviewID'] = [x for x in range (1,6204)]\n",
    "d['rating'] = ans\n",
    "sub = pd.DataFrame(d)\n",
    "sub.to_csv('/data/Out.csv', ',', index=False , columns=['reviewID','rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
